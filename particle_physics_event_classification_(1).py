# -*- coding: utf-8 -*-
"""Particle Physics event classification (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12n2NfZ4zHMLKbbYclLVMsKoNTdD4pX4R
"""

# PARTICLE PHYSICS EVENT CLASSIFICATION

"""# PARTICLE PHYSICS EVENT CLASSIFICATION

## Importing necessary libraries
"""

import os,sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from warnings import filterwarnings
filterwarnings('ignore')


pd.set_option('display.max_columns', None)

df = pd.read_csv('/content/drive/MyDrive/Datasets/Particle_Physics_event_classification.csv')

from google.colab import drive
drive.mount('/content/drive')

"""## Reading the dataset"""

df.head()

df.tail()

print('*'*10,'No. of rows in the dataset are {}'.format(df.shape[0]), '*'*10)
print('*'*10,'No. of columns in the dataset are {}'.format(df.shape[1]),'*'*11)

df.columns.to_list()

df.info()

df.dtypes

df.isnull().sum()  # checking for Null values

df.isna().sum()

df.describe(include='O') # Distribution of categorical variable

"""### Statistical summary of numerical features"""

df.describe().T

df.duplicated().sum()  # Check for duplicate values

# Frequency distribution of Target variable

fig,axes= plt.subplots(nrows=1,ncols=2, figsize=(10,6))
a=df['Label'].value_counts().plot(kind='bar', ax=axes[0])
for i in a.containers:
    a.bar_label(i)
df['Label'].value_counts().plot(kind='pie', ax=axes[1], autopct='%0.1f%%', explode=(0.01,0.01))

# There is 1 feature of integer datatype : 'PRI_jet_num', let's check its distribution

fig, axes = plt.subplots(1,2,figsize=(10,6))
df['PRI_jet_num'].value_counts().plot(kind='pie', autopct= '%0.1f%%', ax=axes[0], explode=(0.01,0.01,0.01,0.01))
b = df['PRI_jet_num'].value_counts().plot(kind='bar', ax=axes[1])
for i in b.containers:
    b.bar_label(i)

# We see many features has -999 values which seems to be imputed in place of either missing or corrupted data.

target=-999
filtered_columns_with999 = df.columns[df.apply(lambda col:any(col == target))]

# Columns that have -999 value are as below:

print('---Features having -999 values are {}\n'.format(filtered_columns_with999))
print('---No. of feature having the value -999 : {}'.format(len(filtered_columns_with999)))

# Table containing percentage and count of -999 in each column:

proportion = pd.DataFrame()
proportion['filtered_columns'] = filtered_columns_with999
proportion['count of -999'] = [df[col].value_counts()[-999] for col in filtered_columns_with999]
proportion['% of -999'] = proportion['count of -999']*100/[len(df[col]) for col in filtered_columns_with999]
proportion.sort_values(by='% of -999', ascending=False, inplace=True)
proportion.reset_index(drop=True, inplace= True)  # We will remove the columns having more than 70% missingness in the data i.e. -999 value.

proportion

# Occurences of -999 in each row :

occurences = (df==-999).sum(axis=1)
occurences

occurences.value_counts()

# Creating a separate dataframe 'data 'where we will be replacing the Target variable with numerical values. 'b' is replaced
# with 0 and 's' is replaced with 1.

data = df.copy()
data['Numerical_label'] = data['Label'].map({'b':0, 's':1})

"""We will also remove the columns 'EventId' and 'Weight' as 'EventId' is not siginificant feature and 'Weight' seems to replicate the nature of the Target variable 'Label' which might overfit our model."""

# The weight of all the particles that are classified as 's' is less than 1.

df[(df['Weight'] >1 ) & (df['Label'] == 's') ]

# There are very few particle classified as 'b' having weight less than 1.

df[(df['Weight'] < 1 ) & (df['Label'] == 'b')]

data.drop(['EventId', 'Weight'], axis=1, inplace=True)

"""To handle the columns having value -999 we will perform univariate analysis of each column by taking into account both the
scenarios i.e. keeping the value and removing the value from the column.
"""

def checkimpact(df,col):
    print(f'\n**********Boxplot with value -999 in the column {col} is as below:','*'*10)
    sns.boxplot(df[col], orient='h', color='red')
    plt.show()
    print('Univarite analysis with value -999:')
    print('Mean is ', df[col].mean(), '\nmedian is', df[col].median(), '\nmax is', df[col].max(), '\nMin is', df[col].min(),
         '\nMode is', df[col].mode()[0], '\nStandard deviation is', df[col].std())
    x = df[df[col]!=-999]
    print('\nBoxplot after removing the value -999 in the column {} is as below: '.format(col))
    sns.boxplot(x[col], orient='h', color='blue')
    plt.show()
    print('Univarite analysis after removing the value -999:')
    print('Mean is ', x[col].mean(), '\nmedian is', x[col].median(), '\nmax is', x[col].max(), '\nMin is', x[col].min(),
         '\nMode is', x[col].mode()[0], '\nStandard deviation is', x[col].std())

for i in filtered_columns_with999:
    checkimpact(df,i)

"""There is huge variation in the features when we drop the value -999 as we can see from the above distribution.
Measures of central tendency, measures of dispersion , min,max values are drastically changing when the value -999
is dropped from the features. Hence we will drop the features having more than 40% of this value.

### Removing the columns having more than 40% of -999 values:
"""

proportion

drop_columns = proportion[proportion['% of -999']>35]['filtered_columns'].values.tolist()
drop_columns

data.drop(drop_columns ,axis=1, inplace=True)

data.shape

remaining_with999 = [print(i) for i in filtered_columns_with999.tolist() if i not in drop_columns]

"""### Imputation for the remaining column:"""

sns.boxplot(data['DER_mass_MMC'], orient='h')
plt.show()

"""The above feature has -999 value. To handle this missingess/extreme outlier we will perform imputation with median as the
distribution seems to be skewed.
"""

data['DER_mass_MMC'] =  data['DER_mass_MMC'].replace(-999, data['DER_mass_MMC'].median())

data.drop('Label', axis=1, inplace=True) # Dropping the categorical Label column as we have already mapped this column to its
                                         # numerical equivalent with column name - 'Numerical_label'

data.shape

# Lets explore which features are highly correlated to each other.
# Here, we will replace -999 value with Nan.

f,ax = plt.subplots(figsize=(20,18))
sns.heatmap(data.corr(), linewidths=.5, fmt='.2f', ax=ax, annot=True)
plt.show()

correlation_data  = data.corr()
correlation_data

threshold=0.8
highly_correlated_pairs_set = set()

for idx, row in correlation_data.iterrows():
    for col, correlation in row.iteritems():
        if abs(correlation) >= threshold and col != idx:
            # Ensure pairs are stored in a consistent order to avoid duplicates
            pair = tuple(sorted([idx, col]))
            highly_correlated_pairs_set.add(pair)
Feature1 = []
Feature2 = []
Correlation = []
corr = pd.DataFrame()
for pair in highly_correlated_pairs_set:
    Feature1.append(pair[0])
    Feature2.append(pair[1])
    Correlation.append(correlation_data.loc[pair[0], pair[1]])
#     print(f"--->{pair[0]}--- {pair[1]}--- , 'correlation =',{ax.loc[pair[0], pair[1]]}")
corr['Feature 1'] = Feature1
corr['Feature 2'] = Feature2
corr['Correlation'] = Correlation
corr.sort_values(by='Correlation' ,ascending=False).reset_index(drop=True)  # Below feature have high correlation :

"""Outlier detection"""

def outliers(data, col):
  mean= data[col].mean()
  std = data[col].std()
  outliers=[]
  for idx, value in enumerate(data[col]):
    z=(value-mean)/std
    if z>3 or z<-3:
      outliers.append(value)
  print('No. of outliers in {} are {}'.format(col, len(outliers)))
  print('Outliers : {}'.format(outliers))

data.columns

outliers(data,'DER_pt_tot')

"""### Separating the Predictor and Target variables

"""

X= data.drop('Numerical_label', axis=1)
y= data['Numerical_label']

X.shape

X.describe()

nrows= 11
ncols= 2
fig, axes = plt.subplots(nrows= nrows, ncols=ncols, figsize=(10,32))
axes = axes.flatten()
for i, column in enumerate(data.columns):
    ax = axes[i]
    sns.histplot(data[column], ax=ax, bins=1 + int(np.log2(len(data))), kde=True) # No. of bins calculated on the basis of Sturges' formula
    ax.set_xlabel(column, fontsize=14)
    ax.set_ylabel('Frequency', fontsize=14)
plt.tight_layout()

## Bivarite Analysis

# We will only take into account those features which have high correlation

corr

high_corr = list(set(corr['Feature 1']).union(set(corr['Feature 2'])))
high_corr

plt.figure(figsize=(20,15))
sns.set(rc={'axes.labelsize': 14})
sns.pairplot(data[high_corr])
plt.show()

data.shape

for column in data.iloc[:,:-1].columns:
    plt.hist([data[data['Numerical_label']==1][column], data[data['Numerical_label']==0][column]],
             label=['Signal','Background'], color=['green','lightblue'])
    plt.xlabel(column)
    plt.ylabel('No. of observations')
    plt.legend()
    plt.show()

# Statistical analysis post handling the missing/corrupt value of the numerical features:

from scipy.stats import skew, kurtosis
for i in data.select_dtypes('float64').columns:
    summary = data[i].describe()
    summary.loc['skewness'] = data[i].skew()
    summary.loc['kurtosis'] = data[i].kurtosis()
    print(summary)
    print('\n')
    print('-'*50)
    print('\n')

data['Numerical_label'].value_counts()

data['DER_pt_h'].describe()

"""### Splitting the data into train and test"""

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test= train_test_split(X,y,test_size=0.20, random_state=101)

y_train.value_counts()

y_test.value_counts()

"""### Scaling the data"""

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()

x_train = ss.fit_transform(x_train)

x_test= ss.transform(x_test)

"""## Model Building

### Decision Tree
"""

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score , classification_report, confusion_matrix, roc_auc_score, roc_curve

def DecisionTree(xtrain,ytrain,xtest,ytest,max_depth,criterion):

    model = DecisionTreeClassifier(max_depth=max_depth,criterion=criterion,random_state=101)
    model.fit(xtrain,ytrain)

    y_pred_test = model.predict(xtest)

    print('*'*10,'Classification report for validation set:','*'*10)
    print(classification_report(ytest,y_pred_test))

    print('*'*10,'F1-score is', round(f1_score(ytest,y_pred_test),2), '*'*10)
    print('\n')

    print('*'*10,'Confusion matrix on validation set:','*'*10)
    plt.figure(figsize=(4,2))
    sns.heatmap(confusion_matrix(ytest,y_pred_test), annot=True, fmt='d')
    plt.show()

    print('\n')
    print('*'*10,'roc_auc_score is : ',round(roc_auc_score(ytest,y_pred_test),2),'*'*10)
    print('\n')

    fpr, tpr, threshold = roc_curve(ytest,y_pred_test)
    print('*'*10,'Receiver Operating Characteristics curve','*'*10)
    plt.figure(figsize=(6,4))
    plt.plot(fpr,tpr,color='indigo')
    plt.xlabel('False Positive rate')
    plt.ylabel('True Positive rate')
    plt.show()

DecisionTree(x_train,y_train,x_test,y_test,4,'gini')  # Base model

# Lets check if the model is overfitting or underfitting

model_dt = DecisionTreeClassifier(max_depth=4, criterion='gini',random_state=101)
model_dt.fit(x_train,y_train)
y_pred_train= model_dt.predict(x_train)
print(classification_report(y_train, y_pred_train))

"""Decision Tree model is not overfitting as there is not much difference in the performance of training and testing dataset.

### Hyperparameter Tuning on Decision Tree model
"""

dt =  DecisionTreeClassifier()
param_grid = {'criterion': ['gini', 'entropy'], 'max_depth' : [5,7,8,10] }
grid_search = GridSearchCV(dt, param_grid=param_grid, cv=5, scoring='accuracy')

# grid_search.fit(x_train,y_train)

# grid_search.best_params_

# Let's leverage these parameters in our model and check the performance:

DecisionTree(x_train,y_train,x_test,y_test,10,'entropy')

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier

def RandomForest(xtrain,ytrain,xtest,ytest,max_depth,criterion):

    model = RandomForestClassifier(max_depth=max_depth,criterion=criterion,random_state=101)
    model.fit(xtrain,ytrain)

    y_pred_test = model.predict(xtest)

    print('*'*10,'Classification report for validation set:','*'*10)
    print(classification_report(ytest,y_pred_test))

    print('*'*10,'F1-score is', round(f1_score(ytest,y_pred_test),2), '*'*10)
    print('\n')

    print('*'*10,'Confusion matrix on validation set:','*'*10)
    plt.figure(figsize=(4,2))
    sns.heatmap(confusion_matrix(ytest,y_pred_test), annot=True, fmt='d')
    plt.show()

    print('\n')
    print('*'*10,'roc_auc_score is : ',round(roc_auc_score(ytest,y_pred_test),2),'*'*10)
    print('\n')

    fpr, tpr, threshold = roc_curve(ytest,y_pred_test)
    print('*'*10,'Receiver Operating Characteristics curve','*'*10)
    plt.figure(figsize=(6,4))
    plt.plot(fpr,tpr,color='indigo')
    plt.xlabel('False Positive rate')
    plt.ylabel('True Positive rate')
    plt.show()

RandomForest(x_train,y_train,x_test,y_test,4,'gini')

# Is the model overfitting?

model_rf = RandomForestClassifier(max_depth=4, criterion='gini', random_state=101)
model_rf.fit(x_train,y_train)
y_pred_train_rf = model_rf.predict(x_train)
print(classification_report(y_train,y_pred_train_rf))

feature_importances = model_rf.feature_importances_

indices = np.argsort(feature_importances[::-1])

X.shape[1]

# Print the feature ranking
print("Feature ranking:")
for f in range(X.shape[1]):
    print(f"{f + 1}.Feature{indices[f]}({data.columns[indices[f]]}): {feature_importances[indices[f]]}")

data.fea

"""The model is not overfitting as there is not much difference in the performance of training and testing dataset."""

# Running the model with criterion 'entropy' and max_depth 7

RandomForest(x_train,y_train,x_test,y_test,7,'entropy')

"""### Aritificial Neural Network (ANN)"""

import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense

classifier = Sequential()

classifier.add(Dense(input_dim=20,activation='relu', units=36))
classifier.add(Dense(activation='relu', units=16))
classifier.add(Dense(activation='sigmoid', units=1))

classifier.summary()

classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = classifier.fit(x_train,y_train,validation_split=0.2,validation_data=(x_test,y_test),epochs=10)

# Model prediction and evaluation

ypred = classifier.predict(x_test)

ypred1=[]
for ele in ypred:
  if ele>0.5:
    ypred1.append(1)
  else:
    ypred1.append(0)

print(accuracy_score(y_test,ypred1)*100)

print(classification_report(y_test,ypred1))

# Training vs Validation accuracy
plt.style.use('ggplot')
plt.plot(history.history['accuracy'],label='train accuracy',color='black')
plt.plot(history.history['val_accuracy'],label='test accuracy',color='red')

plt.title("Training vs Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# Train and Test validation loss plot

plt.style.use('ggplot')
plt.plot(history.history['loss'],label='train loss',color='black')
plt.plot(history.history['val_loss'],label='test loss',color='red')
plt.title("Training vs Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

